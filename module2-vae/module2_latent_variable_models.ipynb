{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "module2_latent_variable_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEcSNKhrotPo",
        "colab_type": "text"
      },
      "source": [
        "# Latent Variable Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7P6yaWwFXOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMx4IKP7dtNT",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: VAEs on 2D Data\n",
        "Here we will train a simple VAE on 2D data, and look at situations in which latents are being used or not being used (i.e. when posterior collapse occurs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtByDqgSB36i",
        "colab_type": "text"
      },
      "source": [
        "## Data\n",
        "We will use 4 datasets, each sampled from some gaussian."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5W1j_KdCSqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_data_1_a(count):\n",
        "    rand = np.random.RandomState(0)\n",
        "    return [[1.0, 2.0]] + (rand.randn(count, 2) * [[5.0, 1.0]]).dot(\n",
        "        [[np.sqrt(2) / 2, np.sqrt(2) / 2], [-np.sqrt(2) / 2, np.sqrt(2) / 2]])\n",
        "\n",
        "\n",
        "def sample_data_2_a(count):\n",
        "    rand = np.random.RandomState(0)\n",
        "    return [[-1.0, 2.0]] + (rand.randn(count, 2) * [[1.0, 5.0]]).dot(\n",
        "        [[np.sqrt(2) / 2, np.sqrt(2) / 2], [-np.sqrt(2) / 2, np.sqrt(2) / 2]])\n",
        "\n",
        "\n",
        "def sample_data_1_b(count):\n",
        "    rand = np.random.RandomState(0)\n",
        "    return [[1.0, 2.0]] + rand.randn(count, 2) * [[5.0, 1.0]]\n",
        "\n",
        "\n",
        "def sample_data_2_b(count):\n",
        "    rand = np.random.RandomState(0)\n",
        "    return [[-1.0, 2.0]] + rand.randn(count, 2) * [[1.0, 5.0]]\n",
        "\n",
        "\n",
        "def q1_sample_data(part, dset_id):\n",
        "    assert dset_id in [1, 2]\n",
        "    assert part in ['a', 'b']\n",
        "    if part == 'a':\n",
        "        if dset_id == 1:\n",
        "            dset_fn = sample_data_1_a\n",
        "        else:\n",
        "            dset_fn = sample_data_2_a\n",
        "    else:\n",
        "        if dset_id == 1:\n",
        "            dset_fn = sample_data_1_b\n",
        "        else:\n",
        "            dset_fn = sample_data_2_b\n",
        "\n",
        "    train_data, test_data = dset_fn(10000), dset_fn(2500)\n",
        "    return train_data.astype('float32'), test_data.astype('float32')\n",
        "\n",
        "\n",
        "def visualize_q1_data(part, dset_id):\n",
        "    train_data, test_data = q1_sample_data(part, dset_id)\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    ax1.set_title('Train Data')\n",
        "    ax1.scatter(train_data[:, 0], train_data[:, 1])\n",
        "    ax2.set_title('Test Data')\n",
        "    ax2.scatter(test_data[:, 0], test_data[:, 1])\n",
        "    print(f'Dataset {dset_id}{part}')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug5jZyP9CXGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize_q1_data('a', 1)\n",
        "visualize_q1_data('a', 2)\n",
        "visualize_q1_data('b', 1)\n",
        "visualize_q1_data('b', 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5X3VHQ2rvCn",
        "colab_type": "text"
      },
      "source": [
        "Consruct and train a VAE with the following characteristics\n",
        "*   2D latent variables $z$ with a standard normal prior, $p(z) = N(0, I)$\n",
        "*   An approximate posterior $q_\\theta(z|x) = N(z; \\mu_\\theta(x), \\Sigma_\\theta(x))$, where $\\mu_\\theta(x)$ is the mean vector, and $\\Sigma_\\theta(x)$ is a diagonal covariance matrix\n",
        "*   A decoder $p(x|z) = N(x; \\mu_\\phi(z), \\Sigma_\\phi(z))$, where $\\mu_\\phi(z)$ is the mean vector, and $\\Sigma_\\phi(z)$ is a diagonal covariance matrix\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "\n",
        "\n",
        "1.   Over the course of training, record the average full negative ELBO, reconstruction loss $E_xE_{z\\sim q(z|x)}[-\\log{p(x|z)}]$, and KL term $E_x[D_{KL}(q(z|x)||p(z))]$ of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
        "2.   Report the final test set performance of your final model\n",
        "3. Samples of your trained VAE with ($z\\sim p(z), x\\sim N(x;\\mu_\\phi(z),\\Sigma_\\phi(z))$) and without ($z\\sim p(z), x = \\mu_\\phi(z)$) decoder noise\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3qice-vN65o",
        "colab_type": "text"
      },
      "source": [
        "## Solution\n",
        "Fill out the functions below, create additional classes/functions/cells if needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vgy646yRI4Ny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict, defaultdict\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class VAEModel(nn.Module):\n",
        "    def loss(self, x):\n",
        "        \"\"\"\n",
        "        returns dict with losses (loss_name -> loss_value)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def sample(self, n, noise=True):\n",
        "        \"\"\"\n",
        "        returns numpy array of n sampled points, shape=(n, 2)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class FullyConnectedVAE(VAEModel):\n",
        "    # YOUR CODE HERE (define encoder & decoder in __init__)\n",
        "\n",
        "    def loss(self, x):\n",
        "        mu_z, log_std_z = # YOUR CODE\n",
        "        z = # YOUR CODE\n",
        "        mu_x, log_std_x = # YOUR CODE\n",
        "\n",
        "        # Compute reconstruction loss\n",
        "        recon_loss = # YOUR CODE\n",
        "\n",
        "        # Compute KL\n",
        "        kl_loss = # YOUR CODE\n",
        "\n",
        "        loss = recon_loss + kl_loss\n",
        "        return {\"loss\": loss, \"reconstruction_loss\": recon_loss, \"kl_loss\": kl_loss}\n",
        "\n",
        "    def sample(self, n, noise=True):\n",
        "        # YOUR CODE\n",
        "\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, epoch, grad_clip=None):\n",
        "    \"\"\"\n",
        "    train model on loader for single epoch\n",
        "    returns Dict[str, List[float]] - dict of losses on each training batch\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    losses = defaultdict(list)\n",
        "    # YOUR CODE\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "def valid_epoch(model, data_loader):\n",
        "    \"\"\"\n",
        "    evaluates model on dataset\n",
        "    returns Dict[str, float] - dict with average losses on entire dataset\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    # YOUR CODE\n",
        "    return average_losses\n",
        "\n",
        "\n",
        "def train_loop(model, train_loader, test_loader, epochs=10, lr=1e-3, grad_clip=None):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_losses, test_losses = defaultdict(list), defaultdict(list)\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, epoch, grad_clip)\n",
        "        test_loss = valid_epoch(model, test_loader)\n",
        "\n",
        "        for k in train_loss.keys():\n",
        "            train_losses[k].extend(train_loss[k])\n",
        "            test_losses[k].append(test_loss[k])\n",
        "    return train_losses, test_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOYOUBMRrwTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def q1(train_data, test_data, part, dset_id):\n",
        "    \"\"\"\n",
        "    train_data: An (n_train, 2) numpy array of floats\n",
        "    test_data: An (n_test, 2) numpy array of floats\n",
        "\n",
        "    (You probably won't need to use the two inputs below, but they are there\n",
        "     if you want to use them)\n",
        "    part: An identifying string ('a' or 'b') of which part is being run. Most likely\n",
        "          used to set different hyperparameters for different datasets\n",
        "    dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
        "               used to set different hyperparameters for different datasets\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations, 3) numpy array of full negative ELBO, reconstruction loss E[-log p(x|z)],\n",
        "      and KL term E[KL(q(z|x) | p(z))] evaluated every minibatch\n",
        "    - a (# of epochs + 1, 3) numpy array of full negative ELBO, reconstruciton loss E[-p(x|z)],\n",
        "      and KL term E[KL(q(z|x) | p(z))] evaluated once at initialization and after each epoch\n",
        "    - a numpy array of size (1000, 2) of 1000 samples WITH decoder noise, i.e. sample z ~ p(z), x ~ p(x|z)\n",
        "    - a numpy array of size (1000, 2) of 1000 samples WITHOUT decoder noise, i.e. sample z ~ p(z), x = mu(z)\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" YOUR CODE HERE \"\"\"\n",
        "    model = # YOUR CODE HERE\n",
        "    train_loader = data.DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "    test_loader = data.DataLoader(test_data, batch_size=128)\n",
        "    train_losses, test_losses = train_loop(model, train_loader, test_loader,\n",
        "                                           epochs=10, lr=1e-3)\n",
        "    train_losses = np.stack((train_losses['loss'], train_losses['reconstruction_loss'], train_losses['kl_loss']), axis=1)\n",
        "    test_losses = np.stack((test_losses['loss'], test_losses['reconstruction_loss'], test_losses['kl_loss']), axis=1)\n",
        "\n",
        "    samples_noise = model.sample(1000, noise=True)\n",
        "    samples_nonoise = model.sample(1000, noise=False)\n",
        "\n",
        "    return train_losses, test_losses, samples_noise, samples_nonoise"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uctZR1_0Mn6U",
        "colab_type": "text"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY4OPlg-I-Wi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_2d(samples, title):\n",
        "    plt.scatter(samples[:, 0], samples[:, 1])\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def plot_vae_training_plot(train_losses, test_losses, title):\n",
        "    elbo_train, recon_train, kl_train = train_losses[:, 0], train_losses[:, 1], train_losses[:, 2]\n",
        "    elbo_test, recon_test, kl_test = test_losses[:, 0], test_losses[:, 1], test_losses[:, 2]\n",
        "    plt.figure()\n",
        "    n_epochs = len(test_losses) - 1\n",
        "    x_train = np.linspace(0, n_epochs, len(train_losses))\n",
        "    x_test = np.arange(n_epochs + 1)\n",
        "\n",
        "    plt.plot(x_train, elbo_train, label='-elbo_train')\n",
        "    plt.plot(x_train, recon_train, label='recon_loss_train')\n",
        "    plt.plot(x_train, kl_train, label='kl_loss_train')\n",
        "    plt.plot(x_test, elbo_test, label='-elbo_test')\n",
        "    plt.plot(x_test, recon_test, label='recon_loss_test')\n",
        "    plt.plot(x_test, kl_test, label='kl_loss_test')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()\n",
        "\n",
        "def q1_results(part, dset_id, fn):\n",
        "    print(f\"Dataset {dset_id}{part}\")\n",
        "    train_data, test_data = q1_sample_data(part, dset_id)\n",
        "    train_losses, test_losses, samples_noise, samples_nonoise = fn(train_data, test_data, part, dset_id)\n",
        "    print(f'Final -ELBO: {test_losses[-1, 0]:.4f}, Recon Loss: {test_losses[-1, 1]:.4f}, '\n",
        "          f'KL Loss: {test_losses[-1, 2]:.4f}')\n",
        "    plot_vae_training_plot(train_losses, test_losses, title=f'Dataset {dset_id}{part} Train Plot')\n",
        "    draw_2d(train_data, 'original data')\n",
        "    draw_2d(samples_noise, 'sampled with noise')\n",
        "    draw_2d(samples_nonoise, 'sampled without noise')\n",
        "    \n",
        "q1_results('a', 1, q1)\n",
        "q1_results('a', 2, q1)\n",
        "q1_results('b', 1, q1)\n",
        "q1_results('b', 2, q1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg22J90qROiO",
        "colab_type": "text"
      },
      "source": [
        "## Reflection\n",
        "Compare the sampled xs with and without decoder noise for datasets (a) and (b). For which datasets are the latents being used? Why is this happening (i.e. why are the latents being ignored in some cases)? \n",
        "\n",
        "**Write your answer (1-2 sentences):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAAixSJ1dv7u",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: VAEs on Images\n",
        "\n",
        "After the previous exercise you should understand how to train simple VAE. Now let's move from 2D space to more complex image spaces. The training methodology is just the same, the only difference is if we want to have good results we should have better encoder and decoder models.\n",
        "\n",
        "In this section, you will train different VAE models on image datasets. Execute the cell below to visualize the two datasets (CIFAR10, and [SVHN](http://ufldl.stanford.edu/housenumbers/))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj2CDM5bXBTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.datasets import SVHN, CIFAR10\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "def show_samples(samples, nrow=10, title='Samples'):\n",
        "    samples = (torch.FloatTensor(samples) / 255).permute(0, 3, 1, 2)\n",
        "    grid_img = make_grid(samples, nrow=nrow)\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    plt.imshow(grid_img.permute(1, 2, 0))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "DATA_DIR = './data'\n",
        "def get_cifar10():\n",
        "    train = CIFAR10(root=f'{DATA_DIR}/cifar10', train=True, download=True).data\n",
        "    test = CIFAR10(root=f'{DATA_DIR}/cifar10', train=False).data\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def get_svhn():\n",
        "    train = SVHN(root=f'{DATA_DIR}/svhn', split='train', download=True).data.transpose(0, 2, 3, 1)\n",
        "    test = SVHN(root=f'{DATA_DIR}/svhn', split='test', download=True).data.transpose(0, 2, 3, 1)\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def visualize_cifar10():\n",
        "    _, test = get_cifar10()\n",
        "    samples = test[np.random.choice(len(test), 100)]\n",
        "    show_samples(samples, title=\"CIFAR10 samples\")\n",
        "\n",
        "\n",
        "def visualize_svhn():\n",
        "    _, test = get_svhn()\n",
        "    print(test.shape)\n",
        "    samples = test[np.random.choice(len(test), 100)]\n",
        "    show_samples(samples, title=\"SVHN samples\")\n",
        "\n",
        "visualize_cifar10()\n",
        "visualize_svhn()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aX115gIuMwB",
        "colab_type": "text"
      },
      "source": [
        "## Part (a) VAE\n",
        "In this part, implement a standard VAE with the following characteristics:\n",
        "\n",
        "*   16-dim latent variables $z$ with standard normal prior $p(z) = N(0,I)$\n",
        "*   An approximate posterior $q_\\theta(z|x) = N(z; \\mu_\\theta(x), \\Sigma_\\theta(x))$, where $\\mu_\\theta(x)$ is the mean vector, and $\\Sigma_\\theta(x)$ is a diagonal covariance matrix\n",
        "*   A decoder $p(x|z) = N(x; \\mu_\\phi(z), I)$, where $\\mu_\\phi(z)$ is the mean vector. (We are not learning the covariance of the decoder)\n",
        "\n",
        "You can play around with different architectures and try for better results, but the following encoder / decoder architecture below suffices (Note that image input is always $32\\times 32$.\n",
        "```\n",
        "conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "transpose_conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "linear(in_dim, out_dim)\n",
        "\n",
        "Encoder\n",
        "    conv2d(3, 32, 3, 1, 1)\n",
        "    relu()\n",
        "    conv2d(32, 64, 3, 2, 1) # 16 x 16\n",
        "    relu() \n",
        "    conv2d(64, 128, 3, 2, 1) # 8 x 8\n",
        "    relu()\n",
        "    conv2d(128, 256, 3, 2, 1) # 4 x 4\n",
        "    relu()\n",
        "    flatten()\n",
        "    linear(4 * 4 * 256, 2 * latent_dim)\n",
        "\n",
        "Decoder\n",
        "    linear(latent_dim, 4 * 4 * 128)\n",
        "    relu()\n",
        "    reshape(4, 4, 128)\n",
        "    transpose_conv2d(128, 128, 4, 2, 1) # 8 x 8\n",
        "    relu()\n",
        "    transpose_conv2d(128, 64, 4, 2, 1) # 16 x 16\n",
        "    relu()\n",
        "    transpose_conv2d(64, 32, 4, 2, 1) # 32 x 32\n",
        "    relu()\n",
        "    conv2d(32, 3, 3, 1, 1)\n",
        "```\n",
        "\n",
        "You may find the following training tips helpful\n",
        "*   When computing reconstruction loss and KL loss, average over the batch dimension and **sum** over the feature dimension\n",
        "*   When computing reconstruction loss, it suffices to just compute MSE between the reconstructed $x$ and true $x$ (you can compute the extra constants if you want)\n",
        "*   Use batch size 128, learning rate $10^{-3}$, and an Adam optimizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "\n",
        "\n",
        "1.   Over the course of training, record the average full negative ELBO, reconstruction loss, and KL term of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
        "2.   Report the final test set performance of your final model\n",
        "3. 100 samples from your trained VAE\n",
        "4. 50 real-image / reconstruction pairs (for some $x$, encode and then decode)\n",
        "5. Interpolations of length 10 between 10 pairs of test images from your VAE (100 images total)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_pYz61AfW4U",
        "colab_type": "text"
      },
      "source": [
        "### Solution\n",
        "Fill out the function below and return the neccessary arguments. Feel free to create more cells if need be"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIW7tqSMdwg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def q2_a(train_data, test_data, dset_id):\n",
        "    \"\"\"\n",
        "    train_data: An (n_train, 32, 32, 3) uint8 numpy array of color images with values in {0, ..., 255}\n",
        "    test_data: An (n_test, 32, 32, 3) uint8 numpy array of color images with values in {0, ..., 255}\n",
        "    dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
        "               used to set different hyperparameters for different datasets\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations, 3) numpy array of full negative ELBO, reconstruction loss E[-p(x|z)],\n",
        "      and KL term E[KL(q(z|x) | p(z))] evaluated every minibatch\n",
        "    - a (# of epochs + 1, 3) numpy array of full negative ELBO, reconstruciton loss E[-p(x|z)],\n",
        "      and KL term E[KL(q(z|x) | p(z))] evaluated once at initialization and after each epoch\n",
        "    - a (100, 32, 32, 3) numpy array of 100 samples from your VAE with values in {0, ..., 255}\n",
        "    - a (100, 32, 32, 3) numpy array of 50 real image / reconstruction pairs\n",
        "      FROM THE TEST SET with values in {0, ..., 255}\n",
        "    - a (100, 32, 32, 3) numpy array of 10 interpolations of length 10 between\n",
        "      pairs of test images. The output should be those 100 images flattened into\n",
        "      the specified shape with values in {0, ..., 255}\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" YOUR CODE HERE \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QJvrGXyfnj0",
        "colab_type": "text"
      },
      "source": [
        "### Results\n",
        "Once you've finished `q2_a`, execute the cells below to visualize and save your results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL7s92ynfqil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def q2_results(dset_id, fn):\n",
        "    if dset_id.lower() == 'cifar':\n",
        "        train_data, test_data = get_cifar10()\n",
        "    elif dset_id.lower() == 'svhn':\n",
        "        train_data, test_data = get_svhn()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported dataset\")\n",
        "\n",
        "    train_losses, test_losses, samples, reconstructions, interpolations = fn(train_data, test_data, dset_id)\n",
        "    samples, reconstructions, interpolations = samples.astype('float32'), reconstructions.astype('float32'), interpolations.astype('float32')\n",
        "    print(f'Final -ELBO: {test_losses[-1, 0]:.4f}, Recon Loss: {test_losses[-1, 1]:.4f}, '\n",
        "          f'KL Loss: {test_losses[-1, 2]:.4f}')\n",
        "    plot_vae_training_plot(train_losses, test_losses, f'Dataset {dset_id} Train Plot')\n",
        "    show_samples(samples, title=f'{dset_id} samples')\n",
        "    show_samples(reconstructions, title=f'{dset_id} Reconstructions')\n",
        "    show_samples(interpolations, title=f'{dset_id} Interpolations')\n",
        "\n",
        "q2_results('cifar', q2_a)\n",
        "q2_results('svhn', q2_a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "822YbCb2fsz8",
        "colab_type": "text"
      },
      "source": [
        "## Part (b) VAE with AF Prior\n",
        "In this part, implement a VAE with an Autoregressive Flow prior ([VLAE](https://arxiv.org/abs/1611.02731)) with the following characteristics:\n",
        "\n",
        "*   16-dim latent variables $z$ with a MADE prior, with $\\epsilon \\sim N(0, I)$\n",
        "*   An approximate posterior $q_\\theta(z|x) = N(z; \\mu_\\theta(x), \\Sigma_\\theta(x))$, where $\\mu_\\theta(x)$ is the mean vector, and $\\Sigma_\\theta(x)$ is a diagonal covariance matrix\n",
        "*   A decoder $p(x|z) = N(x; \\mu_\\phi(z), I)$, where $\\mu_\\phi(z)$ is the mean vector. (We are not learning the covariance of the decoder)\n",
        "\n",
        "You can use the same encoder / decoder architectures and training hyperparameters as part (a). For your MADE prior, it would suffice to use two hidden layers of size $512$. More explicitly, your MADE AF (mapping from $z\\rightarrow \\epsilon$) should output location $\\mu_\\psi(z)$ and scale parameters $\\sigma_\\psi(z)$ and do the following transformation on $z$:\n",
        "$$\\epsilon = z \\odot \\sigma_\\psi(z) + \\mu_\\psi(z)$$\n",
        "\n",
        "where the $i$th element of $\\sigma_\\psi(z)$ is computed from $z_{<i}$ (same for $\\mu_\\psi(z)$) and optimize the objective\n",
        "\n",
        "$$-E_{z\\sim q(z|x)}[\\log{p(x|z)}] + E_{z\\sim q(z|x)}[\\log{q(z|x)} - \\log{p(z)}]$$\n",
        "where $$\\log{p(z)} = \\log{p(\\epsilon)} + \\log{\\det\\left|\\frac{d\\epsilon}{dz}\\right|}$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "\n",
        "\n",
        "1.   Over the course of training, record the average full negative ELBO, reconstruction loss, and KL term of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
        "2.   Report the final test set performance of your final model\n",
        "3. 100 samples from your trained VAE\n",
        "4. 50 real-image / reconstruction pairs (for some $x$, encode and then decode)\n",
        "5. Interpolations of length 10 between 10 pairs of test images from your VAE (100 images total)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwT1tOdm0e84",
        "colab_type": "text"
      },
      "source": [
        "### Solution\n",
        "Fill out the function below and return the neccessary arguments. Feel free to create more cells if need be"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpqhOqW1UDby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def q2_b(train_data, test_data, dset_id):\n",
        "    \"\"\"\n",
        "    train_data: An (n_train, 32, 32, 3) uint8 numpy array of color images with values in {0, ..., 255}\n",
        "    test_data: An (n_test, 32, 32, 3) uint8 numpy array of color images with values in {0, ..., 255}\n",
        "    dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
        "               used to set different hyperparameters for different datasets\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations, 3) numpy array of full negative ELBO, reconstruction loss E[-log p(x|z)],\n",
        "      and KL term E[KL(q(z|x) | p(z))] evaluated every minibatch\n",
        "    - a (# of epochs + 1, 3) numpy array of full negative ELBO, reconstruciton loss E[-p(x|z)],\n",
        "      and KL term E[KL(q(z|x) | p(z))] evaluated once at initialization and after each epoch\n",
        "    - a (100, 32, 32, 3) numpy array of 100 samples from your VAE with values in {0, ..., 255}\n",
        "    - a (100, 32, 32, 3) numpy array of 50 real image / reconstruction pairs\n",
        "      FROM THE TEST SET with values in {0, ..., 255}\n",
        "    - a (100, 32, 32, 3) numpy array of 10 interpolations of length 10 between\n",
        "      pairs of test images. The output should be those 100 images flattened into\n",
        "      the specified shape with values in {0, ..., 255}\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" YOUR CODE HERE \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qvP94pm0hYb",
        "colab_type": "text"
      },
      "source": [
        "### Results\n",
        "Once you've finished `q2_b`, execute the cells below to visualize and save your results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPtDMRpf0iAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q2_results('cifar', q2_b)\n",
        "q2_results('svhn', q2_b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aanBbpz5tiRa",
        "colab_type": "text"
      },
      "source": [
        "# Summary\n",
        "\n",
        "In this notebook you implemented simple VAEs, you can also try to implement more complicated models such as VQ-VAE or PixelVAE.\n",
        "\n",
        "To further deepen your knoledge in latent variable models we suggest you to take a look at tasks 3 and 4 in [HW3](https://github.com/rll/deepul/tree/master/homeworks/hw3) of original Berkeley course and try to complete them by yourself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tSIjrqlCAZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}