{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood-based models\n",
    "\n",
    "This seminar will be about likelihood-based models: autoregressive and flow-based. Seminar consists of 3 parts:\n",
    "- Likelihood model in 1D - fitting histogram using SGD\n",
    "- Deep Autoregressive model in 2D\n",
    "- Detaled study of Real NVP model in 2D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. 1D warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will build our first likelihood-based model for 1D data and will try to fit it using gradient methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose your device: don't forget to switch to GPU runtime when working in collab with cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the procedure of data generation. It will generate a dataset of samples $x \\in \\{0 \\dots 99\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data():\n",
    "    count = 10000\n",
    "    rand = np.random.RandomState(0)\n",
    "    a = 0.3 + 0.1 * rand.randn(count)\n",
    "    b = 0.8 + 0.05 * rand.randn(count)\n",
    "    mask = rand.rand(count) < 0.5\n",
    "    samples = np.clip(a * mask + b * (1 - mask), 0.0, 1.0)\n",
    "    \n",
    "    return np.digitize(samples, np.linspace(0.0, 1.0, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate data and perform train/val/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sample_data()\n",
    "train_data, test_data = train_test_split(data, test_size = 0.3)\n",
    "train_data, val_data = train_test_split(train_data, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot and visualize the histogram of training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(data):\n",
    "    counts = Counter(data)\n",
    "    keys = list(counts.keys())\n",
    "    values = list(counts.values())\n",
    "    plt.bar(keys, values)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On lecture we have discussed how to build histogram model. But this model is not the best choice for high-dimensional data. So, we suggesst to you to implement the following parametrized model:\n",
    "\n",
    "$$ p_\\theta(x)_i = \\frac{e^{\\theta_i}}{\\sum_j{e^{\\theta_j}}} $$\n",
    "\n",
    "Where $\\theta=(\\theta_0 \\dots \\theta_{99})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose you to implement this model in the following class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleProbabilityModel(nn.Module):\n",
    "    # Store all parameters of your model as class fields in constructor\n",
    "    def __init__(self,  num_elements=100):\n",
    "        super(SimpleProbabilityModel, self).__init__()\n",
    "        \n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "        \n",
    "    # Forward should return vector of log probabilities for each element\n",
    "    def forward(self):\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "    \n",
    "    # Should sample element using probabilities, obtained from parameters. Return single number 0..99\n",
    "    def sample(self):\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train this model using negative log-likelihood optimization: $ L_i = -\\log p_{y_i} $. Implement this loss calculation for your model given a batch of data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data: n.array of numbers from your training distribution\n",
    "# model: instance of your SimpleProbabilityModel.\n",
    "# should return: negative log-likelihood of your data given the model to perform backpropagation\n",
    "def calc_loss(data, model):\n",
    "    ################\n",
    "    # YOUR CODE HERE\n",
    "    ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create instance of our model and perform training. Note that if your calculated previous loss as classic natural logarithm, here we scale it to binary logarithm for logging likelihood in bits (which is better for interpretation and comparisons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleProbabilityModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simple_model(model, train_data, val_data, num_epochs=10000, batch_size=4000, lr=0.01):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for i in range(num_epochs):\n",
    "        for j in range(len(train_data) // batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            batch = train_data[batch_size * j:batch_size * (j + 1)]\n",
    "            l = calc_loss(batch, model)\n",
    "            train_losses.append(l.item() / math.log(2))\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "        l = calc_loss(val_data, model)\n",
    "        val_losses.append(l.item() / math.log(2))\n",
    "    \n",
    "    print(\"Train NLL(bits)\")\n",
    "    plt.plot(train_losses, color='green')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Val NLL(bits)\")\n",
    "    plt.plot(val_losses, color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Final validation NLL(bits): {}\".format(val_losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_simple_model(model, train_data, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also tune your training parameters (number of epochs, batch size, learning rate, optimizer), to improve validation NLL. You should obtain something below 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's sample values from our model and visualize histograms of our test data and our sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = [model.sample() for _ in range(len(test_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(sampled_data)\n",
    "plot_histogram(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training here might not yield perfect results, but pictures should look at least similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2. 2D discrete data. Autoregressive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will built our own autoregressive model to work with two-dimensional discrete data data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load 2D distribution as is from file. It's 200x200 numpy array with probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_distribution = np.load(\"distribution.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define class to sample pair of numbers $(x,y) \\in \\{0 \\dots 199\\}^2$ from this distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleDist:\n",
    "    def __init__(self, distribution):\n",
    "        self.probabilities = distribution.flatten()\n",
    "        self.rows, self.cols = distribution.shape\n",
    "        self.values = np.array([[i // self.cols, i % self.cols] for i in range(self.rows * self.cols)])\n",
    "\n",
    "    def sample(self):\n",
    "        idx = np.random.choice(self.rows * self.cols, p = self.probabilities)\n",
    "        \n",
    "        return self.values[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we define distribution, sample data and create train/val/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist2D = SampleDist(original_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 100000\n",
    "sampled_data = np.array([dist2D.sample() for _ in range(SIZE)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(sampled_data, test_size = 0.2)\n",
    "train_data, val_data = train_test_split(train_data, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build our autoregressive model in $(x, y)$ as follows:\n",
    "\n",
    "- Train marginal model $p(x)$ as in part 1\n",
    "- Create and train conditional model $p(y|x)$ as multi-layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, create class for your conditional model $p(y|x)$. It should take $x$ as batch of integer inputs and return batch of probability distributions over $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalModel(nn.Module):\n",
    "    # Store all your trainable layers as model fiels in constuctor\n",
    "    def __init__(self):\n",
    "        super(ConditionalModel, self).__init__()\n",
    "        \n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "    \n",
    "    # Forward pass takes LongTensor x of shape (N,) and should return predicted logprobs of shape (N, 200)\n",
    "    def forward(self, x):\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create a model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_model = ConditionalModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cond_model(cond_model, train_data, num_epochs=100, lr=0.001, batch_size=10000):\n",
    "    dataset = torch.utils.data.TensorDataset(torch.LongTensor(train_data.T[0]).to(device), \n",
    "                                             torch.LongTensor(train_data.T[1]).to(device))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    loss = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(cond_model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        for X_train, Y_train in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = cond_model(X_train)\n",
    "            l = loss(predictions, Y_train)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(l.item() / math.log(2))\n",
    "    \n",
    "    print(\"Train NLL(bits)\")\n",
    "    plt.plot(train_losses, color='green')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cond_model(cond_model, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build compound model, we will also need our simple model from part 1, trained on marginal data from our distribution (only x values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_model = SimpleProbabilityModel(num_elements=200).to(device)\n",
    "train_simple_model(x_model, train_data.T[0], val_data.T[0], num_epochs=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to build compound model for our total $(x, y)$ distribution modeling. Having two trained models implement sampling procedure and probability calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompoundModel:\n",
    "    def __init__(self, x_model, cond_model):\n",
    "        self.x_model = x_model\n",
    "        self.cond_model = cond_model\n",
    "        \n",
    "        self.x_model.eval()\n",
    "        self.cond_model.eval()\n",
    "    \n",
    "    # Given two numbers x, y from 0 .. 199, return NLL value -log p(x,y)\n",
    "    # Normalize in in the way it will return NLL in bits / dimention (binary log divided by two in our case)\n",
    "    def get_logprob(self, x, y):\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "    \n",
    "    # Implement sampling procedure. One call should return sample (x, y) as numpy array from two elements\n",
    "    def sample(self):\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_model = CompoundModel(x_model, cond_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate total average NLL in bits / dimension on your validation data. Tune training parameters and conditional model architecture to boost your performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_logprob = 0\n",
    "for elem in val_data:\n",
    "    logprob = compound_model.get_logprob(elem[0], elem[1])\n",
    "    total_logprob += logprob\n",
    "print(\"Total NLL on validation data per dimension: {}\".format(total_logprob / val_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if sampling from your model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_model.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, get enough samples from your final model and display 2D histogram of the results. Compare them with the results you can get from your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_2d_data = np.array([compound_model.sample() for _ in range(test_data.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2dhistogram(data):\n",
    "    plt.hist2d(data.T[0], data.T[1], bins=200, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2dhistogram(sampled_2d_data)\n",
    "plot_2dhistogram(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't this picture resemble anything? (look at the rotated version of the histogram). Check out how your original distribution looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(original_distribution, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. 2D Continuous Data. RealNVP flow model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will build flow model, transforming given 2D continuous distribution to uniform 2D distribution. Our distribution will be represented as a set of samples. We will use RealNVP model, which we discussed on lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we define function for data sampling, use it and perform train/val/test split as usual. This time all elements also have labels, associated with them. We wil use them for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data():\n",
    "    count = 100000\n",
    "    rand = np.random.RandomState(0)\n",
    "    a = [[-1.5, 2.5]] + rand.randn(count // 3, 2) * 0.2\n",
    "    b = [[1.5, 2.5]] + rand.randn(count // 3, 2) * 0.2\n",
    "    c = np.c_[2 * np.cos(np.linspace(0, np.pi, count // 3)),\n",
    "    -np.sin(np.linspace(0, np.pi, count // 3))]\n",
    "\n",
    "    c += rand.randn(*c.shape) * 0.2\n",
    "    data_x = np.concatenate([a, b, c], axis=0)\n",
    "    data_y = np.array([0] * len(a) + [1] * len(b) + [2] * len(c))\n",
    "    perm = rand.permutation(len(data_x))\n",
    "    return data_x[perm], data_y[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = sample_data()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look on a 2D histogram of our train set distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(X_train.T[0], X_train.T[1], bins=100, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualize distribution in point form with different colors, associated with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train.T[0], X_train.T[1], c = Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build RealNVP flow model using affine coupling layer as main building block. In two-coordinates case it takes the following form:\n",
    "\n",
    "$$ (x_1, x_2) \\rightarrow (z_1, z_2) $$\n",
    "$$ z_1 = x_1 $$\n",
    "$$ z_2 = x_2 \\cdot \\sigma(s(x_1)) + t(x_1) $$\n",
    "\n",
    "Where vector from $s, t$ values is calculated as neural network with $x_1$ on input and $\\sigma$ is sigmoid function. Cool thing about RealNVP is that this network can be arbitrary complex and doesn't need to be invertible itself to have invertible flow for affine coupling layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flows support composition and we will use it. In general, we will compose our total flow in the following way:\n",
    "\n",
    "$$ (z_1, z_2) = (\\sigma \\circ f_{\\theta, 1} \\circ \\dots \\circ f_{\\theta, n})(x_1, x_2) $$\n",
    "\n",
    "Where $f_{\\theta, i}$ is affine coupling layers described above. And $\\sigma$ is elementwise sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important thing in flows training is loss function. In case of flows composition, log-likelihood objective takes the following form:\n",
    "\n",
    "$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_i \\log | \\det \\frac{\\partial f_i}{\\partial f_{i-1}}| $$\n",
    "\n",
    "Here $\\det \\frac{\\partial f_i}{\\partial f_{i-1}}$ is Jacobian matrix determinant for each flow operation. And $\\log p_\\theta(z)$ is log likelihood of latent space we are mapping to (in our case $U(0, 1)^2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions (you will need the answers to complete your RealNVP model code):\n",
    "\n",
    "1. What is $\\log | \\det \\frac{\\partial f_i}{\\partial f_{i-1}} |$ for affine coupling operation?\n",
    "2. What is $\\log | \\det \\frac{\\partial f_i}{\\partial f_{i-1}} |$ for elementwise sigmoid operation?\n",
    "3. What are the inverse operations for both sigmoid and affine coupling?\n",
    "4. How $\\log p_\\theta(z)$ in case of $U(0, 1)^2$ can be calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start implementation of RealNVP from affine coupling layer. You will need to implement inverse operation calculations as well as objective update for each layer ($\\log | \\det \\frac{\\partial f_i}{\\partial f_{i-1}} |$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineCoupling(nn.Module):\n",
    "    def __init__(self, num_features=2):\n",
    "        super(AffineCoupling, self).__init__()\n",
    "        # Store your NN model for s and t values calculation\n",
    "        # Your network should take num_features // 2 elements on input (1 in our case)\n",
    "        # And return num_features elements on output (1 in our case)\n",
    "        self.NN = # YOUR CODE HERE\n",
    "    \n",
    "    # Forward pass is implemented for you. Implement your objective update\n",
    "    def forward(self, x, objective):\n",
    "        z1, z2 = torch.chunk(x, 2, dim=1)\n",
    "        h = self.NN(z1)\n",
    "        shift = h[:, 0::2]\n",
    "        \n",
    "        scale = torch.sigmoid(h[:, 1::2])\n",
    "        z2 += shift\n",
    "        z2 *= scale\n",
    "        objective += # YOUR CODE HERE\n",
    "        \n",
    "        return torch.cat([z1, z2], dim=1), objective\n",
    "    \n",
    "    # Implement reverse value calculation\n",
    "    def reverse(self, x):\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for elemntwise sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elementwise sigmoid flow from R^n to (0; 1)^n\n",
    "class ElementwiseSigmoid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ElementwiseSigmoid, self).__init__()\n",
    "    \n",
    "    def forward(self, x, objective):\n",
    "        x = torch.sigmoid(x)\n",
    "        objective += # YOUR CODE HERE\n",
    "        return x, objective\n",
    "    \n",
    "    def reverse(self, x):\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need to swap dimentions from time to time for affine couling layer, so it leaves different parts of input the same on different iterations. Answer the following questions:\n",
    "1. Is this operation legal? Is it a flow? What is the reverse operation?\n",
    "2. That is $\\log | \\det \\frac{\\partial f_i}{\\partial f_{i-1}} |$ for this operation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensions_swap(x):\n",
    "    x1, x2 = torch.chunk(x, 2, dim=1)\n",
    "    return torch.cat([x2, x1], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we build RealNVP model from defined operations. Feel free to change number of affine coupling layers inside the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, n_transforms=4):\n",
    "        super(RealNVP, self).__init__()\n",
    "        \n",
    "        self.affines = nn.ModuleList([AffineCoupling() for _ in range(n_transforms)])\n",
    "        self.sigmoid = ElementwiseSigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        objective = 0\n",
    "        for layer in self.affines:\n",
    "            x, objective = layer(x, objective)\n",
    "            x = dimensions_swap(x)\n",
    "\n",
    "        x, objective = self.sigmoid(x, objective)\n",
    "        \n",
    "        return x, objective\n",
    "    \n",
    "    def reverse(self, x):\n",
    "        x = self.sigmoid.reverse(x)\n",
    "        for layer in reversed(self.affines):\n",
    "            x = dimensions_swap(x)\n",
    "            x = layer.reverse(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, we define training procedure. Here we have calculated our total objective from flows log-det-s, however, we still need to define our loss, which we need to minimize. We also want to accumulate training loss values in bits. Fill this out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_realnvp(model, train_data, batch_size = 512, lr = 0.001, num_epochs = 5):\n",
    "    dataset = torch.utils.data.TensorDataset(torch.Tensor(train_data).to(device))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_losses = []\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        for X_train in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions, objective = model(X_train[0])\n",
    "            \n",
    "            # Define your total loss value here\n",
    "            loss = # YOUR CODE HERE\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(\n",
    "                # Store the value of total NLL here\n",
    "                # YOUR CODE HERE\n",
    "            )\n",
    "            \n",
    "    print(\"Train NLL(bits)\")\n",
    "    plt.plot(train_losses, color='green')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model and launch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realnvp = RealNVP().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_realnvp(realnvp, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate your model: calculate NLL in bits on your validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realnvp.eval()\n",
    "\n",
    "# Implement NLL calculation on X_val data\n",
    "################\n",
    "# YOUR CODE HERE\n",
    "###############\n",
    "val_nll_bits = # YOUR CODE HERE\n",
    "print(\"NLL on validation set in bits: {}\".format(val_nll_bits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to tune your model parameters to improve your NLL score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time for some visualizations! Let's calculate flow values of our validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_flow = realnvp(torch.Tensor(X_val).to(device))[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will visualize our points before and after passing throw the flow, leaving the label color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_val.T[0], X_val.T[1], c = Y_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sampled_flow.T[0], sampled_flow.T[1], c = Y_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's perform sampling from RealNVP and visualize their histogram to compare with testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_realnvp(model):\n",
    "    x1 = random.random()\n",
    "    x2 = random.random()\n",
    "    latent = torch.Tensor([[x1, x2]]).to(device)\n",
    "    result = realnvp.reverse(latent)[0]\n",
    "    \n",
    "    return result.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = [sample_from_realnvp(realnvp) for _ in range(X_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = np.array(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(sampled_data.T[0], sampled_data.T[1], bins=100, cmap='gray',range=[[-3, 3], [-2, 3]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(X_test.T[0], X_test.T[1], bins=100, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seminar doesn't cover high-dimensional case: working with distribution on images. However, it's not so different from 2D case. You are free to implement and try to run RealNVP or PixelCNN models on real images datasets: MNIST, CIFAR, CelebA.\n",
    "\n",
    "To further deepen your knoledge in likelihood-based models we suggest you to take a look at task 3 in <a href=\"https://drive.google.com/open?id=1DtYllaV4Yk8ljgYcLBmdXNplEDTG6HT6\">HW1</a> and task 2 in <a href=\"https://drive.google.com/file/d/1xs9fFCrPs3c9HNnOlmgen1ZnLfs26VVM/view?usp=sharing\">HW2</a> of original Berkeley course and try to complete them by yourself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
